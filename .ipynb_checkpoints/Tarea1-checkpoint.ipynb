{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Inicialización de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "import time\n",
    "from IPython import display\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "\n",
    "# Dígito a identificar. Fija el dígito verificador de tu RUT (K=0).\n",
    "RUT_veri_number = 1\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "minibatch_size = 32\n",
    "n_epochs = 100\n",
    "n_training_samples = 55000 # Max: 55000\n",
    "target_cost = 0.01\n",
    "\n",
    "# Parámetros de Gradient Descent\n",
    "learning_rate = 0.1\n",
    "momentum = 0.0\n",
    "\n",
    "# Número de neuronas\n",
    "n_inputs = 28*28\n",
    "n_hidden = 25\n",
    "n_classes = 2\n",
    "n_neurons = np.hstack((n_inputs,n_hidden,n_classes)).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de base de datos MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "\n",
    "training_images = mnist.train.images[0:n_training_samples,:]\n",
    "training_labels = mnist.train.labels[0:n_training_samples,:]\n",
    "training_labels = training_labels[:,RUT_veri_number]\n",
    "\n",
    "validation_images = mnist.validation.images\n",
    "validation_labels = mnist.validation.labels\n",
    "validation_labels = validation_labels[:,RUT_veri_number]\n",
    "\n",
    "testing_images = mnist.test.images\n",
    "testing_labels = mnist.test.labels\n",
    "testing_labels = testing_labels[:,RUT_veri_number]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción de MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_input = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "previous_layer = MLP_input\n",
    "\n",
    "# Pointer to weights and biases for regularization\n",
    "regularizers = 0\n",
    "\n",
    "# Iteration constructing one layer at a time\n",
    "for level in range(len(n_neurons)-1):\n",
    "    weights = tf.Variable(tf.random_uniform   ([n_neurons[level],n_neurons[level+1]], minval= -np.sqrt(3/n_neurons[level]),maxval=np.sqrt(3/n_neurons[level])))\n",
    "    #weights = tf.Variable(tf.truncated_normal([n_neurons[level],n_neurons[level+1]], stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(0., shape=[n_neurons[level+1]]))\n",
    "    applied_weights = tf.matmul(previous_layer, weights) + biases\n",
    "\n",
    "    regularizers += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "\n",
    "    if level < len(n_neurons)-2:\n",
    "        layer = tf.sigmoid(applied_weights)\n",
    "        previous_layer = layer\n",
    "    else:\n",
    "        MLP = tf.nn.softmax(applied_weights)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Función de costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accomodate target\n",
    "target = tf.placeholder(tf.float32, shape=[None])\n",
    "one_hot_target = tf.one_hot(tf.cast(target, dtype=tf.int32), 2)\n",
    "\n",
    "# Cross Entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(one_hot_target * tf.log(tf.clip_by_value(MLP,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "# Mean Squared Error\n",
    "mean_squared_error = tf.sqrt(tf.reduce_mean(tf.square(target - MLP[:,1])))\n",
    "\n",
    "\n",
    "cost_function = mean_squared_error\n",
    "#cost_function = cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularización "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cost_function += 0.0005 * regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent with Momentum\n",
    "GDM_optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "\n",
    "# Adam Algorithm\n",
    "Adam_optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "training_algorithm = GDM_optimizer.minimize(cost_function)\n",
    "#training_algorithm = Adam_optimizer.minimize(cost_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del método"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "n_minibatches = int(np.shape(training_images)[0] / minibatch_size) \n",
    "\n",
    "cost_valid_history = []\n",
    "cost_train_history = []\n",
    "\n",
    "prev_cost_valid = 10.0\n",
    "validation_checks = 0\n",
    "max_validation_checks = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i_mb in range(n_minibatches):\n",
    "        a,b = i_mb*minibatch_size, (i_mb+1)*minibatch_size\n",
    "        images_minibatch = training_images[a:b,:]\n",
    "        labels_minibatch = training_labels[a:b]\n",
    "        sess.run(training_algorithm,feed_dict={MLP_input: images_minibatch, target: labels_minibatch})\n",
    "        \n",
    "    cost_valid = sess.run(cost_function,feed_dict={MLP_input: validation_images, target: validation_labels})\n",
    "    cost_valid_history += [cost_valid]\n",
    "    \n",
    "    cost_train = sess.run(cost_function,feed_dict={MLP_input: training_images, target: training_labels})\n",
    "    cost_train_history += [cost_train]\n",
    "    \n",
    "    if prev_cost_valid < cost_valid:\n",
    "        validation_checks += 1\n",
    "    else:\n",
    "        validation_checks = 0\n",
    "        prev_cost_valid = cost_valid\n",
    "        \n",
    "    print(\"Epoch: %d/%d, Training cost: %f, Validation cost: %f, Validation checks: %d/%d\" %(epoch+1, n_epochs,cost_train,cost_valid,validation_checks,max_validation_checks))\n",
    "    \n",
    "    if cost_train <= target_cost:\n",
    "        print 'Target cost reached'\n",
    "        break\n",
    "    if validation_checks >= max_validation_checks:\n",
    "        print 'Early stopping'\n",
    "        break\n",
    "    \n",
    "    \n",
    "print ''\n",
    "    \n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(cost_valid_history, label=\"Validation\")\n",
    "plt.plot(cost_train_history, label=\"Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost history\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Test MLP\n",
    "\n",
    "print 'Training results:'\n",
    "predicted_training_labels = sess.run(MLP[:,1],feed_dict={MLP_input: training_images})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(training_labels.astype(bool),(predicted_training_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "print 'Validation results:'\n",
    "predicted_validation_labels = sess.run(MLP[:,1],feed_dict={MLP_input: validation_images})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(validation_labels.astype(bool),(predicted_validation_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "print 'Test results:'\n",
    "predicted_test_labels = sess.run(MLP[:,1],feed_dict={MLP_input: testing_images})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(testing_labels.astype(bool),(predicted_test_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "fpr, tpr, th = roc_curve(testing_labels, predicted_test_labels)\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot2grid((1,2),(0,0))\n",
    "plt.plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "plt.subplot2grid((1,2),(0,1))\n",
    "plt.plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"False Negative Rate\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.title('DET Curve')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
